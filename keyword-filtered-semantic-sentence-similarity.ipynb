{"cells":[{"metadata":{"id":"ziMua5htbBa-"},"cell_type":"markdown","source":"#### Create an Embeddings Index\n\nThis Script produces an embedding index that allows a researcher to perform a semantic similarity search on the cord-19 dataset. It has a few features\n\n- You can search an index (Faiss) for sentences with semantic similarity to an input query. \n    - The sentences are constructued using sentence embeddings (SentenceTransformer library)\n    - SentenceTransformer is initialised with covid-bert-base, and then fine-tuned with NLI and STS tasks so it adds semantic components to the underlying covid based language model. \n    \n- A search filter function allows you to limit the documents compiled into the semantic index, by keyword; \n    - If the keyword appears in the documents precompiled \"list of entities\", the document is inclued in the index. \n    - Entities(e.g. compounds, treatments, protocols) are detected with scipacy,\n\nBriefly, the following steps are performed.  . \n\n- Report \n\n- Part A\n    - Step 1: Initialise SentenceTransfomer model with covid_bert_base (https://huggingface.co/deepset/covid_bert_base)\n    - Step 2: Fine-tune SentenceTransformer model with natural language inference (sentence entailment) and semantic similarity (sts) tasks\n \n\n - Part B \n\n    - This part is divided into Part B.1 and Part B.2\n\n    - Part B.1 is run on Kaggle kernel. It generates a dataframe with columns paper_id, abstract and body_text.\n\n    - Part B.2 was run on Google Colab, which connects to a GCP VM, with 16 cores to allow efficient multiprocessin- g of dataframe batches. . I included the code here to view. This part takes the dataframe from B.1, and uses scispacy to extract scientific entities, and creates a new column \"ents\" in the dataframe. \n\n-  Part C \n -   Create Faiss Index for each sentence within corpus \n  \n\n## TO DO\nstill missing some abstracts which means filtering should look at first seciton of body_text for ents...\n\nsearch function should be refactored, nlp(doc) should be factored out so its only performed once for each doc \n\nmemory usage is too high. Need to rethink use of garbage collector, or more memory efficient table storage\n\n\n\n"},{"metadata":{"trusted":true,"id":"PYkh4MvgbBbD"},"cell_type":"markdown","source":"## References \n\n#### Kaggle Kernels \n\n-  https://www.kaggle.com/xhlulu/cord-19-eda-parse-json-and-generate-clean-csv\n- https://www.kaggle.com/maksimeren/covid-19-literature-clustering\n\n\nThese kernels provided code cells cut+pasted into this notebook. They have been to used generate a clean dataset through parsing json files. Many thanks. \n\n\n#### Spacy\n\n- https://spacy.io/\n\nSpacy is used to perform nlp pipeline functions such as tokenization, sentence segmentation and span retrieval\n\n#### deepset/covid-bert-base\n\n- https://huggingface.co/deepset/covid_bert_base\n- https://github.com/deepset-ai/FARM/blob/master/examples/lm_finetuning.py \n\nDeepsetAI script showing how to fine-tune BERT language model with a language modeling task. From what I can gather, they used script lm_finetuning.py in their FARM tools using the CORD-19 dataset to fine-tune the model. \n\n#### SentenceTransformers\n- https://github.com/UKPLab/sentence-transformers\n- https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_nli_bert.py\n\nAfter loading the covid-bert model, we continued to fine-tune the sentence-transformer model to perform well of natural language tasks, since we will be performing these sorts of tasks when runnning our searches on the embeddings-index. \n\n#### Faiss\n- https://gist.github.com/mdouze/e30e8f57a98ed841c082cc68baa14b4a\nThis provides code to serialise and deserialise the index so it can be pickled. "},{"metadata":{"id":"phB2ypewbBbS"},"cell_type":"markdown","source":"### Setup and Installation  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display full output in Jupyter notebook cell (not only final statement)\nfrom __future__ import print_function\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# torch from pytorch (huggingface)\nimport torch\n#from transformers import *\nimport pandas as pd\nimport numpy as np\nimport os\nimport pickle\nfrom pathlib import Path\n\nimport collections\nfrom tqdm import tqdm\nimport pprint\n\ntqdm.pandas(desc=\"my bar!\")","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"id":"Ew206-JMbBbp","outputId":"d079b835-cb2d-4190-a865-036d3d9c0253","_kg_hide-output":true},"cell_type":"code","source":"# if using GPU, test GPU is working\n\ncuda = torch.device('cuda')     # Default CUDA device\ntorch.__version__\ntorch.cuda.get_device_name(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"yeHblkEGbBb0","outputId":"5229b456-c8b1-4a73-c91b-21293388ddfc"},"cell_type":"code","source":"# set display options\npd.options.display.max_rows\npd.set_option('display.max_colwidth', -1)\nfrom IPython.display import display, HTML\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.set_option('display.expand_frame_repr', False)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"60"},"metadata":{}}]},{"metadata":{"trusted":true,"id":"PrClvpT3bBb8"},"cell_type":"code","source":"%%capture\n# clone sentence-transformers code and examples, and install\n# install sentence-transformers and download repo to perform fine-tuning steps. pip install will not give you access to all the examples\n\n!git clone https://github.com/UKPLab/sentence-transformers.git\nos.chdir(\"/kaggle/working/sentence-transformers\")\n!pip install -e .","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"ttI10cbPbBcG"},"cell_type":"code","source":"%%capture\n# Install spacy and scispacy, and scispacy language model\n!pip install scispacy\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Rlx0wCY7bBcP"},"cell_type":"code","source":"import scispacy\nimport spacy\nimport en_core_sci_sm\nnlp = en_core_sci_sm.load()\nnlp.max_length=100000000 # for extra long documents","execution_count":null,"outputs":[]},{"metadata":{"id":"0oudbwc_nuXg","trusted":true},"cell_type":"code","source":"#!pip install sentence-transformers\n#import os\n#os.chdir(\"/content/sentence-transformers\")\nfrom sentence_transformers import SentenceTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"5MQeRxMobBcU"},"cell_type":"code","source":"\n%%capture\n#!pip install faiss-cpu\nuseGPU=True\n\n!pip install faiss-gpu\nimport faiss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"id":"a-efpFiMbBcZ"},"cell_type":"code","source":"# define some generic functions\n\n# define sentence_embedding function\ndef embed_sentence_list(model,list_of_sents):\n    sentence_embeddings=model.encode(list_of_sents)\n    doc_matrix=np.asarray(sentence_embeddings,dtype=np.float32)\n    return(doc_matrix)\n\n\n# we need to serialise faiss index to save it to output\n#https://gist.github.com/mdouze/e30e8f57a98ed841c082cc68baa14b4a\n\ndef serialize_index(index):\n    \"\"\" convert an index to a numpy uint8 array  \"\"\"\n    writer = faiss.VectorIOWriter()\n    faiss.write_index(index, writer)\n    return faiss.vector_to_array(writer.data)\n\n\ndef deserialize_index(data):\n    reader = faiss.VectorIOReader()\n    faiss.copy_array_to_vector(data, reader.data)\n    return faiss.read_index(reader)","execution_count":null,"outputs":[]},{"metadata":{"id":"C6jWPzZobBce"},"cell_type":"markdown","source":"### End of Setup\n"},{"metadata":{"id":"z4WRhmuxbBcf"},"cell_type":"markdown","source":"## Report: Semantic Search for limited documents \n\nThis section contains the report, and assumes Part A, B and C have been run. We can load pre-built objects from those sections from the input directory.\n"},{"metadata":{},"cell_type":"markdown","source":"### First,** load our fine-tuned SentenceTransformer model (created in Part A below)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_load_path='/kaggle/input/bertcovidbasicnlists/training_nli_sts_covid-bert-base-2020-04-01_00-26-48'\n\nif useGPU:\n    model=torch.load(model_load_path) \n\nelse: # not working\n    #model=torch.load(model_load_path,map_location=torch.device('cpu')) \n    device = torch.device('cpu')\n    model=SentenceTransformer()\n    model.load_state_dict(torch.load(model_load_path, map_location=device) # not working\n# type(model) #  model is of type \"SentenceTransformer\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Second, load our processed CORD-19 dataset (see Part B)\nFormat of dataframe:\n\n- [paper_id, abstract, body_text, ents]\n\npaper_id - paper_id of the original paper\nabstract - text of the abstract\nbody_text - text of the body \nents - a list of entities extracted from the abstract (or body_text, if abstract missing) with scipy entity recognition\n\nSee Part B details. Work largely cut+paste cells from: https://www.kaggle.com/maksimeren/covid-19-literature-clustering"},{"metadata":{"trusted":true,"id":"6_8RG9GFbBcv"},"cell_type":"code","source":"# Load pre-built files containing dataframes, where each row represents one paper.\n# Columns are [paper_id, abstract_text, body_text,ents]\n# The \"ents\" column represents entities present in the abstract (or if not provided, the body text.) \n# The \"ents\" column was generate using code in Part B, but was run on Google Colab as described below. \ndef read_full_cord19_df():\n    df_covid1=pd.read_csv(\"/kaggle/input/cord19-df-with-entities/1.csv\",index_col=0)\n    df_covid2=pd.read_csv(\"/kaggle/input/cord19-df-with-entities/2.csv\",index_col=0)\n    df_covid3=pd.read_csv(\"/kaggle/input/cord19-df-with-entities/3.csv\",index_col=0)\n    df_covid4=pd.read_csv(\"/kaggle/input/cord19-df-with-entities/4.csv\",index_col=0)\n    df_covid5=pd.read_csv(\"/kaggle/input/cord19-df-with-entities/5.csv\",index_col=0)\n\n    # Concatenate individual dfs to one df\n    df_covid_ents=pd.concat([df_covid1,df_covid2,df_covid3,df_covid4,df_covid5])\n    return df_covid_ents","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"uRftDafUbBc0"},"cell_type":"code","source":"# Read in processed Document dataframe \ndf_covid_ents=read_full_cord19_df()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Third, read in our pre-trained gpu index, and associated dictionary\n"},{"metadata":{"trusted":true,"id":"srHrakPnbBcf"},"cell_type":"code","source":"# Load pre-built faiss index from input\n# This contains an embedding for each sentence, for each document\n# The sentence embedding model has been trained as per Part A, below. \n\ndef read_full_faiss_index():\n    filepath=Path(\"/kaggle/input/faiss-index-file-full-d\") / \"index_faiss_file_all\"\n    file=open(filepath, \"rb\")\n    data=pickle.load(file)\n    faiss_index=deserialize_index(data)\n\n    # convert index from cpu to gpu\n    gpu_index=None\n    if useGPU==True:\n        res = faiss.StandardGpuResources()  # use a single GPUres = faiss.StandardGpuResources()  # use a single GPU\n        gpu_index = faiss.index_cpu_to_gpu(res, 0, faiss_index)\n    \n    # Load pre-built index dictionary\n    # format, for each index in faiss index (key), values are the paper_id, and sentence_id, of the embedded sentence at that index locaiton in the faiss index. \n\n    filepath = Path(\"/kaggle/input/faiss-index-to-doc-sent-ids-dict\") / \"faiss_index_ids_dict_all\"\n    #filepath=\"/content/drive/My Drive/kaggle/covid19/input/faiss_index_ids_dict_all\"\n\n    infile=open(filepath, \"rb\")\n    ids_dict=pickle.load(infile)\n    \n    return gpu_index, ids_dict, faiss_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"choZcQVCbBcl"},"cell_type":"code","source":"gpu_index, ids_dict, faiss_index=read_full_faiss_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test Faiss Index Search with Step by Step Query \n\n### Step 1: Create and Embed Input Query\nGenerate a test query for semantic similarity search.\n\n\nThe faiss engine does an inner product on the query embeddings against the indexed embeddings, so we must normalise the query embeddings first. (The indexed embeddings were normalised upon creation. \n"},{"metadata":{"trusted":true,"id":"h1xuj_ZNbBc5","_kg_hide-output":true},"cell_type":"code","source":"# Let us run a test queries\nquery_list=[]\nquery_list.append(\"nurse to patient transmission in aged care facilities\")\n\n# create a numpy array, and normalise\nxq=embed_sentence_list(model,query_list) # model is instantiated previously from sentence-transformers\nfaiss.normalize_L2(xq)","execution_count":null,"outputs":[]},{"metadata":{"id":"8AdMwa9HbBc_"},"cell_type":"markdown","source":"### Step 2 - Generate Top K Semantic Search Results\nWe can now generate the top K semantic matches. This returns the top k best sentences, with the closest semantic meaning to our query sentence. \n\nThe object \"top_k\" is returned as a list with 2 elements. The first element contains the cosine simmilarity scores of the matches, and the second element contains the ids (row number) of the match in the faiss index. \n\nIt returns one row per query. Within each row, columns are the 1..k'th best match for the query."},{"metadata":{"trusted":true,"id":"q7ydifJKbBdC","outputId":"2833ac1e-631b-46e0-c576-23500f95bde6"},"cell_type":"code","source":"top_k = gpu_index.search(xq[0,:].reshape(1,-1), 3) # sanity check\ntop_k","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### Step 3 Examine the Results\n\nWe can examine the scores and index of the top sentence matches, returned from the search of faiss. \nHowever, we still need to retrieve details of the original sentence, to see whether our match makes any sense. "},{"metadata":{"trusted":true,"id":"wcbGsDX2bBdH","outputId":"91a6adfc-366c-4a08-dcfa-ea4259684071"},"cell_type":"code","source":"j=0 # first result\n\n#  return faiss index values from matches from second element in result list (at top_k[1]).\n# Once in list, get first row (query), and j'th column (j'th best match ) (at ...to_list()[0][j])\nindex_tmp=top_k[1].tolist()[0][j] \n\n# Retrieve original sentence text from best match \npaper_id, sent_id=ids_dict[index_tmp][0] # use dictionary to retrieve original paper, and sentence location \npaper_row=df_covid_ents[df_covid_ents['paper_id']==paper_id] # get row in document by filtering for unique paper id \n\ndoc=nlp(paper_row.iloc[0]['body_text']) # convert text to sentences \n\n\n# get sentence - need to re-execute spacy pipeline to retrieve sentences since this is not stored. \nlist_of_sents=[sent.text for sent in doc.sents]\nsent=list_of_sents[sent_id] # retrieve sentence id\n\n\nprint(sent)","execution_count":null,"outputs":[]},{"metadata":{"id":"NL5JtJYibBdM"},"cell_type":"markdown","source":"That's great. The sentence looks relevant. \n\nBut we might need more context to know whether that sentence is relevant to our query. We use spacy spans to grab the context, i.e. text around the sentence. "},{"metadata":{},"cell_type":"markdown","source":"#### Step 4, Generate Sentence \"Context\""},{"metadata":{"trusted":true,"id":"8w4ZS5AibBdN","outputId":"e962a7a9-18bf-4509-bd8b-452c97de4771"},"cell_type":"code","source":"list_of_spans=[sent for sent in doc.sents]\n\nspan_start=list_of_spans[sent_id].start\nspan_end=list_of_spans[sent_id].end\nif span_start < 100:\n    span_start = 0\nelse:\n    span_start -=100\nif (span_end + 100) > len(doc):\n    span_end = len(doc)\nelse:\n    span_end += 100\n    \nspan_results=doc[span_start: span_end]\n\n\nlist_of_spans[sent_id] # original sentence \nspan_results # full span ","execution_count":null,"outputs":[]},{"metadata":{"id":"LkK14WT-bBdW"},"cell_type":"markdown","source":"### Step 5, Double-check cosine similiarity score manually \n\nLet's check the score returned by faiss manually. It should be the cosine between the embedded query matrix, and the result vector. \n"},{"metadata":{"trusted":true,"id":"ibVumgHFbBdX","outputId":"51c6cb2b-68e0-4cd8-dad3-da51903afe13"},"cell_type":"code","source":"score_tmp=top_k[0].tolist()[0][j] # return faiss scores from first element in result list (at top_k[0])\nscore_tmp # faiss inner product score\n\n# check the score manually, to make sure it is what we expect \ndoc_matrix=embed_sentence_list(model,list_of_sents)\nfaiss.normalize_L2(doc_matrix)\nfrom scipy.spatial.distance import cosine\n1-cosine(xq[0,:],doc_matrix[sent_id,:]) # first query (this is a cosine distance, not a cosine similarity)  # agreement! \n\nnp.inner(xq[0,:],doc_matrix[sent_id,:]) # agreement! ","execution_count":null,"outputs":[]},{"metadata":{"id":"giCVL1EjbBdd"},"cell_type":"markdown","source":"### Functions to generate new index based on a subset of documents\n#### Filtered by Keyword\n\nNow, we might need to create our own index, based on a limited set of keywords. \nThis is a bit time-consuming, and ideally I would shift this to a GCP VM, with an API where it might run more quickly."},{"metadata":{"trusted":true,"id":"kBO9wNutbBdf"},"cell_type":"code","source":"pp = pprint.PrettyPrinter(indent=4)\n\ndef create_gpu_index(df_covid_tmp):\n    \"\"\" \n    used to generate a smaller faiss index, limited by keyword\n    \"\"\"\n    tqdm.pandas(desc=\"my bar!\")\n\n    print(\"creating new gpu index of size\", df_covid_tmp.shape[0])\n    \n    # init faiss index\n    d=768 # sentence transformer embedding length\n    res = faiss.StandardGpuResources()  # use a single GPU\n    index = faiss.IndexIDMap(faiss.IndexFlatIP(d)) # IP is inner product. Data must be normalised first\n    gpu_index_tmp = faiss.index_cpu_to_gpu(res, 0, index)\n\n    # init dict\n    ids_dict_tmp = collections.defaultdict(list)\n\n    ids_next=0\n    i=0\n    for row in tqdm(range(df_covid_tmp.shape[0])):\n        doc=nlp(df_covid_tmp.iloc[row]['body_text'])\n        paper_id=df_covid_tmp.iloc[row]['paper_id']\n        \n        list_of_sents=[sent.text for sent in doc.sents]\n        #if len(list_of_sents) > 800:\n        #    list_of_sents=list_of_sents[:800] # just taking first 800 sentences for memory reasons. \n            \n        doc_matrix=embed_sentence_list(model,list_of_sents)\n        faiss.normalize_L2(doc_matrix)\n\n        # ids in faiss index begin after last idx (last document processed, last sentence)\n        custom_ids = np.array(range(ids_next, ids_next+len(doc_matrix))) # from last postion (range add +1) to new position\n        ids_next=ids_next+len(doc_matrix) # increment by current document length. Current doc lenght = num sentences in current doc             \n        gpu_index_tmp.add_with_ids(doc_matrix, custom_ids)\n        for sent_idx, faiss_ids_val in enumerate(custom_ids): # sentence_idx is sentence id within the 1 document, faiss_ids_val is the faiss index value\n            items=(paper_id, sent_idx)\n            ids_dict_tmp[faiss_ids_val].append(items)\n            \n    del doc_matrix\n    del doc\n    gc.collect()\n    return(gpu_index_tmp,ids_dict_tmp)","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"f87pRaE-bBdl"},"cell_type":"code","source":"# to  do - add in function to process search response\n# combine with absolute match result to ensure sentence contain keywords\npp = pprint.PrettyPrinter(indent=4)\nimport gc\ndef search(model,query, k=3, gpu_index_default=None,ids_dict_default=None,keyword=None,df_covid_ents_default=None):\n    \"\"\"\n    returns results based on keyword, the gpu_index, and the ids_dict\n    \n    :param model, query, k, keyword, gpu_index_default, ids_dict_default \n    :type\n    :param query\n    :type \n    \"\"\"\n         \n    query_list=[query]\n    xq=embed_sentence_list(model,query_list) # model is instantiated previously from sentence-transformers\n    faiss.normalize_L2(xq)\n\n    if df_covid_ents_default is None:\n      df_covid_ents=read_full_cord19_df() #df_covid_ents\n    else:\n      df_covid_ents=df_covid_ents_default\n\n    if gpu_index_default==None or ids_dict_default==None:\n\n      if keyword is not None:\n        df_covid_tmp=df_covid_ents[df_covid_ents['ents'].str.contains(keyword, na=False) ].copy() # filter on keyword\n        print(\"Create new faiss index based on\", df_covid_tmp.shape[0], \"documents\")\n        gpu_index_tmp,ids_dict_tmp = create_gpu_index(df_covid_tmp)  # generate new index, based on filtered dataframe\n\n      else: # no keyword but still need to create index from scratch\n        gpu_index_tmp, ids_dict_tmp = read_full_faiss_index() #gpu_index_default # if no keyword, use original artifacts\n        df_covid_tmp=df_covid_ents #df_covid_ents\n\n    else: \n        print(\"using custom index and dict - ignoring any keywords\") #ignore keyword\n        gpu_index_tmp=gpu_index_default\n        ids_dict_tmp=ids_dict_default\n        df_covid_tmp=df_covid_ents\n  \n    # get top k best matches through index look_up\n    top_k = gpu_index_tmp.search(xq, k)  \n    # >>>\n    pp.pprint(\"end faiss search\")\n    # >>>\n      \n    # init output results dataframe \n    colnames=[\"query\",\"sentence\",\"score\", \"span\",\"paper_id\"]    \n    results=pd.DataFrame(columns=colnames)\n      \n    for i, _id in tqdm(enumerate(top_k[1].tolist()[0])): # for each result in the top k, element zero since only one query (could do batch queries)\n        paper_id, sent_id=ids_dict_tmp[_id][0]\n          \n        ## retrieve sent value   \n        # get row in document by filtering for unique paper id \n        paper_row=df_covid_tmp[df_covid_tmp['paper_id']==paper_id]\n        \n        # convert text to sentences \n        doc=nlp(paper_row.iloc[0]['body_text'])\n\n        # get sentence - need to re-execute spacy pipeline to retrieve sentences since this is not stored. \n        list_of_sents=[sent.text for sent in doc.sents]\n        sentence_result=list_of_sents[sent_id] # retrieve original sentence text       \n\n        # retrieve spans\n        list_of_spans=[sent for sent in doc.sents]\n        \n        span_start=list_of_spans[sent_id].start\n        span_end=list_of_spans[sent_id].end\n\n        if span_start < 100:\n          span_start = 0\n        else:\n          span_start -= 100\n                \n        if (span_end + 100) > len(doc):\n          span_end = len(doc)\n        else:\n          span_end += 100\n                \n        span_results=doc[span_start: span_end]        \n\n        # get score\n        score_tmp=top_k[0].tolist()[0][i] # score for i'th match\n            \n        #title=paper_row['title'] # to do\n        #authors=paper_row['authors'] # to \n        #abstract=paper_row['abstract'] # to do\n            \n        tmp=pd.DataFrame( [pd.Series([query,sentence_result, score_tmp, span_results, paper_id],index=colnames)] )\n        results=results.append(tmp, ignore_index=True, sort=False)\n              \n    return(results, gpu_index_tmp, ids_dict_tmp)\n","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"0vZiHJvbbBdr"},"cell_type":"markdown","source":"\n## Example: Simple Semantic Search \n\nLets now do a query on all the data (full index) for sentences that shed light on nurse transmission to patients in hospitals. "},{"metadata":{"trusted":true,"id":"IvMXYj8WbBds","outputId":"b4047782-3f34-4154-9760-3778cdb06c05"},"cell_type":"code","source":"# if GPU is working, run search \n\nresults=search(model=model,\n               query=\"nurses transmit to patients in hospitals\", \n               k=6)\ndf,index_,dict_=results\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"DYJndAxtbBd6","outputId":"bf35518d-6a4e-4b79-8c91-c4977c14c13a"},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/results-output/full_dataset_results_transmission.csv\") # pregenerate results\n\ndf[[\"paper_id\",\"sentence\",\"score\",\"span\"]].head() ","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"                                   paper_id                                                                                                                                                                                                        sentence     score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  span\n0  1b44eb4e309e0a843a7c308eeddab0e852704a19  First, nurses come in contact with their patients more frequently than doctors while performing their basic nursing and life care tasks.                                                                        0.762037  . Therefore, examining the PTSD symptoms of these medical workers must be prioritized. Exposure to traumatic events is the immediate cause of PTSD and is essential in diagnosing such disorder. Poton [16] found that medical workers usually manifest the symptoms of their patients after experiencing a traumatic event, but such symptoms were always ignored. Alexander [17] suggested that those people who were repeatedly exposed to traumatic events were prone to suffering all kinds of psychological problems.\\nThe nurses obtained higher PCL-C scores than the doctors for several reasons. First, nurses come in contact with their patients more frequently than doctors while performing their basic nursing and life care tasks. Second, many nurses have limited knowledge on H7N9. Third, nurses feel nervous, panicky, and helpless when facing epidemics. Moreover, as reported in the literature, females are 2.38e2.49 times more likely to develop lifetime PTSD than males after their exposure to similar traumatic events [18] . Accordingly, the female respondents in this study showed higher PTSD levels than the male respondents.\\nThe younger nurses received higher PCL-C scores than their older counterparts. Specifically, those nurses with work experience of less than three years have limited knowledge\n1  f79cb6c46151a0af6270805ed459f6e9bc95e031  First, nurses come in contact with their patients more frequently than doctors while performing their basic nursing and life care tasks.                                                                        0.762037  . Therefore, examining the PTSD symptoms of these medical workers must be prioritized. Exposure to traumatic events is the immediate cause of PTSD and is essential in diagnosing such disorder. Poton [16] found that medical workers usually manifest the symptoms of their patients after experiencing a traumatic event, but such symptoms were always ignored. Alexander [17] suggested that those people who were repeatedly exposed to traumatic events were prone to suffering all kinds of psychological problems.\\nThe nurses obtained higher PCL-C scores than the doctors for several reasons. First, nurses come in contact with their patients more frequently than doctors while performing their basic nursing and life care tasks. Second, many nurses have limited knowledge on H7N9. Third, nurses feel nervous, panicky, and helpless when facing epidemics. Moreover, as reported in the literature, females are 2.38e2.49 times more likely to develop lifetime PTSD than males after their exposure to similar traumatic events [18] . Accordingly, the female respondents in this study showed higher PTSD levels than the male respondents.\\nThe younger nurses received higher PCL-C scores than their older counterparts. Specifically, those nurses with work experience of less than three years have limited knowledge\n2  954d000b2e7252ffb05812341a6436d3d784494a  Nurses in the 7th floor who had close contact with MERS patients and showed MERS symptoms were transferred (National medical center, Chungju medical center, Gonju medical center, and Daejeon Army Hospital).  0.739247  ward because of small numbers of admitting rooms. Medical staffs exposed to MERS worked in the cohort area.\\nOn the June 18th, a nurse working in the cohort ward was confirmed with MERS due to the 119th patient. RRT recommended emergency room and out-patient clinic be closed after coordinating with hospital officials in order to prepare isolating rooms. Patients in the cohort ward were sent to a single room. Patients who didn`t have close contact on the 5th floor were discharged. Wards on the 3rd floor modified to single isolated room. Nurses in the 7th floor who had close contact with MERS patients and showed MERS symptoms were transferred (National medical center, Chungju medical center, Gonju medical center, and Daejeon Army Hospital). Also Army medical staffs were dispatched due to lack of medical staffs.\\nOn June 13th, an information technology (IT) employee who worked at the ' A' hospital (the 143th MERS patient) was identified on the 12th floor. The hospital was closed and the RRT guided isolate all patients on the 11th and 12th floor. Non-contact patients were discharged or transferred to another hospital. Single isolation rooms were prepared on the 11th and 12th floor. Contact medical staffs were quarantined, and the 180th patient was identified 8 days                   \n3  98e23bb77cb986fe21f095253af64843d3f4e8cc  Recognized cases occurred predominantly among staff in hospitals and nursing homes.                                                                                                                             0.723472  to HCWs (6 cases), and from HCWs to their household contacts (1 case). Five out of six occupational cases of ebola hemorrhagic fever occurred in HCWs [27] .\\nAn immunocompetent HCW with no known history of varicellazoster virus disease was exposed to a patient with herpes zoster and was immunized 2 days later. Twenty-seven days after receiving the varicella vaccine, while hospitalized, she developed a disseminated rash [28] . Occupationally acquired infection with methicillin-resistant Staphylococcus aureus (MRSA) is an issue of increasing concern. Recognized cases occurred predominantly among staff in hospitals and nursing homes. The most frequent infection sites were ears, nose, and throat, followed by skin. Only in a few cases, a genetic link between an MRSA-infected index patient and MRSA in an HCW was documented. MRSA infection was recognized as an occupational disease due to known contact with MRSA-positive patients or because workplace conditions were presumed to involve increased exposure to MRSA. As recognition of HCWs often depends on workplace characteristics the surveillance of MRSA infections in HCWs would facilitate the recognition of MRSA infections as an occupational disease [29] .\\nNosocomial                                                                        \n4  0573f2143ac48f91fc247c875388e55d8576bcab  Recognized cases occurred predominantly among staff in hospitals and nursing homes.                                                                                                                             0.723472  to HCWs (6 cases), and from HCWs to their household contacts (1 case). Five out of six occupational cases of ebola hemorrhagic fever occurred in HCWs [27] .\\nAn immunocompetent HCW with no known history of varicellazoster virus disease was exposed to a patient with herpes zoster and was immunized 2 days later. Twenty-seven days after receiving the varicella vaccine, while hospitalized, she developed a disseminated rash [28] . Occupationally acquired infection with methicillin-resistant Staphylococcus aureus (MRSA) is an issue of increasing concern. Recognized cases occurred predominantly among staff in hospitals and nursing homes. The most frequent infection sites were ears, nose, and throat, followed by skin. Only in a few cases, a genetic link between an MRSA-infected index patient and MRSA in an HCW was documented. MRSA infection was recognized as an occupational disease due to known contact with MRSA-positive patients or because workplace conditions were presumed to involve increased exposure to MRSA. As recognition of HCWs often depends on workplace characteristics the surveillance of MRSA infections in HCWs would facilitate the recognition of MRSA infections as an occupational disease [29] .\\nNosocomial                                                                        ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paper_id</th>\n      <th>sentence</th>\n      <th>score</th>\n      <th>span</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1b44eb4e309e0a843a7c308eeddab0e852704a19</td>\n      <td>First, nurses come in contact with their patients more frequently than doctors while performing their basic nursing and life care tasks.</td>\n      <td>0.762037</td>\n      <td>. Therefore, examining the PTSD symptoms of these medical workers must be prioritized. Exposure to traumatic events is the immediate cause of PTSD and is essential in diagnosing such disorder. Poton [16] found that medical workers usually manifest the symptoms of their patients after experiencing a traumatic event, but such symptoms were always ignored. Alexander [17] suggested that those people who were repeatedly exposed to traumatic events were prone to suffering all kinds of psychological problems.\\nThe nurses obtained higher PCL-C scores than the doctors for several reasons. First, nurses come in contact with their patients more frequently than doctors while performing their basic nursing and life care tasks. Second, many nurses have limited knowledge on H7N9. Third, nurses feel nervous, panicky, and helpless when facing epidemics. Moreover, as reported in the literature, females are 2.38e2.49 times more likely to develop lifetime PTSD than males after their exposure to similar traumatic events [18] . Accordingly, the female respondents in this study showed higher PTSD levels than the male respondents.\\nThe younger nurses received higher PCL-C scores than their older counterparts. Specifically, those nurses with work experience of less than three years have limited knowledge</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f79cb6c46151a0af6270805ed459f6e9bc95e031</td>\n      <td>First, nurses come in contact with their patients more frequently than doctors while performing their basic nursing and life care tasks.</td>\n      <td>0.762037</td>\n      <td>. Therefore, examining the PTSD symptoms of these medical workers must be prioritized. Exposure to traumatic events is the immediate cause of PTSD and is essential in diagnosing such disorder. Poton [16] found that medical workers usually manifest the symptoms of their patients after experiencing a traumatic event, but such symptoms were always ignored. Alexander [17] suggested that those people who were repeatedly exposed to traumatic events were prone to suffering all kinds of psychological problems.\\nThe nurses obtained higher PCL-C scores than the doctors for several reasons. First, nurses come in contact with their patients more frequently than doctors while performing their basic nursing and life care tasks. Second, many nurses have limited knowledge on H7N9. Third, nurses feel nervous, panicky, and helpless when facing epidemics. Moreover, as reported in the literature, females are 2.38e2.49 times more likely to develop lifetime PTSD than males after their exposure to similar traumatic events [18] . Accordingly, the female respondents in this study showed higher PTSD levels than the male respondents.\\nThe younger nurses received higher PCL-C scores than their older counterparts. Specifically, those nurses with work experience of less than three years have limited knowledge</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>954d000b2e7252ffb05812341a6436d3d784494a</td>\n      <td>Nurses in the 7th floor who had close contact with MERS patients and showed MERS symptoms were transferred (National medical center, Chungju medical center, Gonju medical center, and Daejeon Army Hospital).</td>\n      <td>0.739247</td>\n      <td>ward because of small numbers of admitting rooms. Medical staffs exposed to MERS worked in the cohort area.\\nOn the June 18th, a nurse working in the cohort ward was confirmed with MERS due to the 119th patient. RRT recommended emergency room and out-patient clinic be closed after coordinating with hospital officials in order to prepare isolating rooms. Patients in the cohort ward were sent to a single room. Patients who didn`t have close contact on the 5th floor were discharged. Wards on the 3rd floor modified to single isolated room. Nurses in the 7th floor who had close contact with MERS patients and showed MERS symptoms were transferred (National medical center, Chungju medical center, Gonju medical center, and Daejeon Army Hospital). Also Army medical staffs were dispatched due to lack of medical staffs.\\nOn June 13th, an information technology (IT) employee who worked at the ' A' hospital (the 143th MERS patient) was identified on the 12th floor. The hospital was closed and the RRT guided isolate all patients on the 11th and 12th floor. Non-contact patients were discharged or transferred to another hospital. Single isolation rooms were prepared on the 11th and 12th floor. Contact medical staffs were quarantined, and the 180th patient was identified 8 days</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>98e23bb77cb986fe21f095253af64843d3f4e8cc</td>\n      <td>Recognized cases occurred predominantly among staff in hospitals and nursing homes.</td>\n      <td>0.723472</td>\n      <td>to HCWs (6 cases), and from HCWs to their household contacts (1 case). Five out of six occupational cases of ebola hemorrhagic fever occurred in HCWs [27] .\\nAn immunocompetent HCW with no known history of varicellazoster virus disease was exposed to a patient with herpes zoster and was immunized 2 days later. Twenty-seven days after receiving the varicella vaccine, while hospitalized, she developed a disseminated rash [28] . Occupationally acquired infection with methicillin-resistant Staphylococcus aureus (MRSA) is an issue of increasing concern. Recognized cases occurred predominantly among staff in hospitals and nursing homes. The most frequent infection sites were ears, nose, and throat, followed by skin. Only in a few cases, a genetic link between an MRSA-infected index patient and MRSA in an HCW was documented. MRSA infection was recognized as an occupational disease due to known contact with MRSA-positive patients or because workplace conditions were presumed to involve increased exposure to MRSA. As recognition of HCWs often depends on workplace characteristics the surveillance of MRSA infections in HCWs would facilitate the recognition of MRSA infections as an occupational disease [29] .\\nNosocomial</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0573f2143ac48f91fc247c875388e55d8576bcab</td>\n      <td>Recognized cases occurred predominantly among staff in hospitals and nursing homes.</td>\n      <td>0.723472</td>\n      <td>to HCWs (6 cases), and from HCWs to their household contacts (1 case). Five out of six occupational cases of ebola hemorrhagic fever occurred in HCWs [27] .\\nAn immunocompetent HCW with no known history of varicellazoster virus disease was exposed to a patient with herpes zoster and was immunized 2 days later. Twenty-seven days after receiving the varicella vaccine, while hospitalized, she developed a disseminated rash [28] . Occupationally acquired infection with methicillin-resistant Staphylococcus aureus (MRSA) is an issue of increasing concern. Recognized cases occurred predominantly among staff in hospitals and nursing homes. The most frequent infection sites were ears, nose, and throat, followed by skin. Only in a few cases, a genetic link between an MRSA-infected index patient and MRSA in an HCW was documented. MRSA infection was recognized as an occupational disease due to known contact with MRSA-positive patients or because workplace conditions were presumed to involve increased exposure to MRSA. As recognition of HCWs often depends on workplace characteristics the surveillance of MRSA infections in HCWs would facilitate the recognition of MRSA infections as an occupational disease [29] .\\nNosocomial</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Example - Keyword Filtered Document Set - Semantic Similarity Search \nNow, lets do a refined search. Lets limit the search to only those documents that contain \"hydroxychloroquine\" within the extracted list of \"entities\". This will give < 100 documents. We can then compile a limited faiss index from this shortlist of documents. **"},{"metadata":{"trusted":true,"id":"GUs9LynrbBeF","outputId":"7a86ce4b-a491-481f-8a7c-829946820191"},"cell_type":"code","source":"# works if Kaggle GPU is working\nresults=search(model=model,\n               query=\"timing intervals of treatment delivered\",\n               k=3, \n               gpu_index_default=None, \n               ids_dict_default=None,\n                 keyword=\"hydroxychloroquine\", \n)\ndf_results_hcq, gcu_index_hcq, ids_dict_hcq=results\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"8Mw6RSVCbBeK","outputId":"730ef2ae-df0c-4c2b-a1ff-5a02db1aa323"},"cell_type":"code","source":"# read in results compiled on Google Colab manually for now (since Kaggle GPU unavailable)\ndf_results_hcq=pd.read_csv(\"/kaggle/input/results-output/df_results_hcq.csv\")\ndf_results_hcq[['paper_id','sentence','score','span']]","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"     paper_id                                                                  sentence     score                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        span\n0  PMC3654146  Prognosis depends on timing of institution of therapy.\\n                  0.840111  Moderate to severe episode which our patient had is defined as involvement of at least two or more lobes along with decreased oxygen saturation require full red-cell exchange transfusion. Goal is to reduce hemoglobin S to less than 30 % while keeping hemoglobin to less than 10 gm/dl. Full exchange transfusion appears to be more iron-neutral than partial exchange transfusion. Exchange transfusion can be performed manually or by apheresis machine. Third generation cephalosporin and a macrolide can be safely used in pregnancy to cover for atypical bacteria as well as streptococcus pneumonia and hemophilus influenzae. Prognosis depends on timing of institution of therapy.\\nACUTE HEPATIC FAILURE IN ADULT-ONSET STILL’S DISEASE - A RARE MANIFESTATION: DRAMATIC RESPONSE TO PULSE METHYLPREDNISOLONE THERAPY Nalini Valluru; Michael Windham; Venkata S. Tammana; Eyasu Mekonen. Howard University Hospital, Washington, DC. (Tracking ID #1645499)\\nLEARNING OBJECTIVE 1: Manage potentially fatal hepatic failure in the setting of Adult Onset Still’s Disease\\nCASE: We present a case of 22-year-old woman with no significant medical history who initially came with fever, arthralgias, myalgias and sore throat. On examination she was noted to have\n1  PMC4429500  Interval between discharge and the follow up visit were calculated.       0.701133  study period, and who saw their PCP for follow up.\\nMETHODS: WCIMA is a large internal medicine practice affiliated with Weill Cornell Medical College in New York City. It is the main practice site for 33 faculty physicians and 136 internal medicine residents. Through data extraction of our electronic health records, we identified all WCIMA discharges from NYP/WCMC from 9/1/2013 to 10/12/2013. Information for each discharge including basic patient identifiers, age, payer, hospital department/ service, assigned primary care provider, completed visit date and provider were obtained. Interval between discharge and the follow up visit were calculated. The unit of analysis was a hospital discharge event, some patients had multiple hospitalizations and ED/urgent care visits. IBM SPSS software version 22 procedures for data manipulation and descriptive statistics were used for data analysis. IRB approval is pending.\\nRESULTS: We identified 1,908 unique discharges for 1,513 WCIMA patients from NYP/WCMC during the study period. Two hundred ninety-three patients (19.4 %) had more than one discharge. In WCIMA patients discharged from a Medicine admission, 51.2 % (N = 259/506) had a post discharge visit at WCIMA,                      \n2  PMC4429500  We stratified analyses for each intervention category by outcome timing.  0.692260  ), home visits, telemonitoring, telephone support, or interventions to increase provider continuity. We required studies to report a readmission rate, mortality rate, or the composite outcome (all-cause readmission or mortality). We included outcomes occurring no longer than 6 months following an index hospitalization. Two investigators independently selected, extracted data from, and rated risk of bias of included studies. We grouped studies of similar interventions for our evidence synthesis based on the mode and environment of delivery. We used random-effects models to estimate pooled effects. We stratified analyses for each intervention category by outcome timing. We calculated the number needed to treat (NNT) for readmission and mortality outcomes when appropriate. We graded strength of evidence (SOE) based on established guidance.\\nRESULTS: We included 47 trials. Most included people with moderate to severe HF; mean ages were in the 70s. Few trials reported 30-day readmission rates. At 30 days, high intensity home-visiting programs reduced all-cause readmission and the composite endpoint (all-cause readmission or death) (low SOE). Over 3 to 6 months, home-visiting programs reduced all-cause readmission,              ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>paper_id</th>\n      <th>sentence</th>\n      <th>score</th>\n      <th>span</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PMC3654146</td>\n      <td>Prognosis depends on timing of institution of therapy.\\n</td>\n      <td>0.840111</td>\n      <td>Moderate to severe episode which our patient had is defined as involvement of at least two or more lobes along with decreased oxygen saturation require full red-cell exchange transfusion. Goal is to reduce hemoglobin S to less than 30 % while keeping hemoglobin to less than 10 gm/dl. Full exchange transfusion appears to be more iron-neutral than partial exchange transfusion. Exchange transfusion can be performed manually or by apheresis machine. Third generation cephalosporin and a macrolide can be safely used in pregnancy to cover for atypical bacteria as well as streptococcus pneumonia and hemophilus influenzae. Prognosis depends on timing of institution of therapy.\\nACUTE HEPATIC FAILURE IN ADULT-ONSET STILL’S DISEASE - A RARE MANIFESTATION: DRAMATIC RESPONSE TO PULSE METHYLPREDNISOLONE THERAPY Nalini Valluru; Michael Windham; Venkata S. Tammana; Eyasu Mekonen. Howard University Hospital, Washington, DC. (Tracking ID #1645499)\\nLEARNING OBJECTIVE 1: Manage potentially fatal hepatic failure in the setting of Adult Onset Still’s Disease\\nCASE: We present a case of 22-year-old woman with no significant medical history who initially came with fever, arthralgias, myalgias and sore throat. On examination she was noted to have</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PMC4429500</td>\n      <td>Interval between discharge and the follow up visit were calculated.</td>\n      <td>0.701133</td>\n      <td>study period, and who saw their PCP for follow up.\\nMETHODS: WCIMA is a large internal medicine practice affiliated with Weill Cornell Medical College in New York City. It is the main practice site for 33 faculty physicians and 136 internal medicine residents. Through data extraction of our electronic health records, we identified all WCIMA discharges from NYP/WCMC from 9/1/2013 to 10/12/2013. Information for each discharge including basic patient identifiers, age, payer, hospital department/ service, assigned primary care provider, completed visit date and provider were obtained. Interval between discharge and the follow up visit were calculated. The unit of analysis was a hospital discharge event, some patients had multiple hospitalizations and ED/urgent care visits. IBM SPSS software version 22 procedures for data manipulation and descriptive statistics were used for data analysis. IRB approval is pending.\\nRESULTS: We identified 1,908 unique discharges for 1,513 WCIMA patients from NYP/WCMC during the study period. Two hundred ninety-three patients (19.4 %) had more than one discharge. In WCIMA patients discharged from a Medicine admission, 51.2 % (N = 259/506) had a post discharge visit at WCIMA,</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PMC4429500</td>\n      <td>We stratified analyses for each intervention category by outcome timing.</td>\n      <td>0.692260</td>\n      <td>), home visits, telemonitoring, telephone support, or interventions to increase provider continuity. We required studies to report a readmission rate, mortality rate, or the composite outcome (all-cause readmission or mortality). We included outcomes occurring no longer than 6 months following an index hospitalization. Two investigators independently selected, extracted data from, and rated risk of bias of included studies. We grouped studies of similar interventions for our evidence synthesis based on the mode and environment of delivery. We used random-effects models to estimate pooled effects. We stratified analyses for each intervention category by outcome timing. We calculated the number needed to treat (NNT) for readmission and mortality outcomes when appropriate. We graded strength of evidence (SOE) based on established guidance.\\nRESULTS: We included 47 trials. Most included people with moderate to severe HF; mean ages were in the 70s. Few trials reported 30-day readmission rates. At 30 days, high intensity home-visiting programs reduced all-cause readmission and the composite endpoint (all-cause readmission or death) (low SOE). Over 3 to 6 months, home-visiting programs reduced all-cause readmission,</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"id":"Eeeo2ZlZyJD3","outputId":"5122df08-4088-46d1-92dd-0f322ec101ce","trusted":false},"cell_type":"code","source":"# NOTE - we could also run, passing in a pre-compiled index for a given keyword (\"hydroxychloroquine\") in this case. \nresults=search(model=model,\n               query=\"the treatment was delivered at timing intervals \",\n               k=3, \n               gpu_index_default=gcu_index_hcq, \n               ids_dict_default=ids_dict_hcq,\n               keyword=None, df_covid_ents_default=df_covid_ents\n)","execution_count":0,"outputs":[]},{"metadata":{"id":"gu2bkyiM4jvY","outputId":"b66ec36a-3928-4e28-f3d2-eef2557b3222","trusted":false},"cell_type":"code","source":"df_results_hcq_timing, _, _ = results\ndf_results_hcq_timing[['paper_id','sentence','score','span']]","execution_count":null,"outputs":[]},{"metadata":{"id":"-z-QhyMVbBeN"},"cell_type":"markdown","source":"### Part A - SentenceTransformer model fine-tuning "},{"metadata":{"trusted":true,"id":"pNZImS9IbBeO"},"cell_type":"code","source":"# import sentence_transformer functions \n# See fine-tuning example at - https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_nli_bert.py\nos.chdir(\"/kaggle/working/sentence-transformers\")\n\nfrom sentence_transformers import models, losses\nfrom sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer\nfrom sentence_transformers.readers import *\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\nfrom sentence_transformers.datasets import *\n\nfrom torch.utils.data import TensorDataset, DataLoader, SequentialSampler\nfrom torch.utils.data import DataLoader\nimport math\nimport logging\nfrom datetime import datetime","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"60RIwW-TbBeR"},"cell_type":"markdown","source":"Deepset have finetuned a BERT language model on the CORD-19 dataset (see references above). \n\nhttps://huggingface.co/deepset/covid_bert_base\n\nWe load this model. \nThe following code is otherwise lifted from \"https://github.com/UKPLab/sentence-transformers/blob/master/examples/training_nli_bert.py\" to complete the remainder of the fine-tuning. \n    "},{"metadata":{"trusted":true,"id":"4jGjQ4lybBeR"},"cell_type":"code","source":"# set up SentenceTransformer model, using pooled averaging. \n# start with covid_bert_base model, rather than CORD-19 task\n\nword_embedding_model = models.BERT(\"deepset/covid_bert_base\")\n\n# Apply mean pooling to get one fixed sized sentence vector\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n                               pooling_mode_mean_tokens=True,\n                               pooling_mode_cls_token=False,\n                               pooling_mode_max_tokens=False)\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"7xdOeIeBbBeV"},"cell_type":"code","source":"# Now fine-tune model further on NLI and STS tasks to get natural language and semantic tones included in embeddings\n\n!python examples/datasets/get_data.py # downloads AllNLI.zip and STSbenchmark.zip datasets ","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"JRHfckTvbBeY"},"cell_type":"code","source":"os.chdir(\"./examples\") # switch to examples directory where output is stored\nnli_reader = NLIDataReader('datasets/AllNLI')\ntrain_data = SentencesDataset(nli_reader.get_examples('train.gz'), model=model)\nmodel_name = 'covid-bert-base'\nbatch_size = 16\nnli_reader = NLIDataReader('datasets/AllNLI')\nsts_reader = STSDataReader('datasets/stsbenchmark')\ntrain_num_labels = nli_reader.get_num_labels()\nmodel_save_path = 'kaggle/working/training_nli_'+model_name+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n\ntrain_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntrain_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=train_num_labels)\n\nlogging.info(\"Read STSbenchmark dev dataset\")\ndev_data = SentencesDataset(examples=sts_reader.get_examples('sts-dev.csv'), model=model)\ndev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=batch_size)\nevaluator = EmbeddingSimilarityEvaluator(dev_dataloader)\n\n# Configure the training\nnum_epochs = 1\n\nwarmup_steps = math.ceil(len(train_dataloader) * num_epochs / batch_size * 0.1) #10% of train data for warm-up\nlogging.info(\"Warmup-steps: {}\".format(warmup_steps))\n\n# Train the model\nmodel.fit(train_objectives=[(train_dataloader, train_loss)],\n          evaluator=evaluator,\n          epochs=num_epochs,\n          evaluation_steps=1000,\n          warmup_steps=warmup_steps,\n          output_path=model_save_path\n          )","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"fCozyCjSbBea"},"cell_type":"code","source":"# save model \nmodel_save_name='training_nli_covid_'+model_name+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\nfilename=Path(\"/kaggle/working/\" ) / model_save_name \noutfile = open(filename,'wb')\ntorch.save(model, outfile)","execution_count":0,"outputs":[]},{"metadata":{"id":"LC-O9ZqibBed"},"cell_type":"markdown","source":"### Part B - Create df_covid \n\nThis part is divided into Part B.1 and Part B.2\n\nPart B.1 was run on Kaggle kernel. It generates a dataframe with columns paper_id, abstract and body_text.\n\nPart B.2 was run on Google Colab, which connects to a GCP VM, with 16 cores to allow efficient multiprocessing of dataframe batches. . I included the code here to view. This part takes the dataframe from B.1, and uses scispacy to extract scientific entities, and creates a new column \"ents\" in the dataframe. "},{"metadata":{"trusted":true,"id":"-dc4oHNDbBee"},"cell_type":"code","source":"#https://www.kaggle.com/ivanegapratama/covid-eda-initial-exploration-tool\n# papers_df only got stuff from biox...\n# we need to get the full data set \n\n#https://www.kaggle.com/ivanegapratama/covid-eda-initial-exploration-tool\n    \n#import matplotlib.pyplot as plt\n#plt.style.use('ggplot')\nimport glob\nimport json\n\nroot_path = '/kaggle/input/CORD-19-research-challenge'\nmetadata_path = f'{root_path}/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\n#meta_df.head()","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"grbZubG2bBeh"},"cell_type":"code","source":"all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\nlen(all_json)","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"nzwau0UrbBej"},"cell_type":"code","source":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            if 'abstract' in content.keys():\n                for entry in content['abstract']:\n                    self.abstract.append(entry['text'])\n            else:\n                self.abstract.append('')\n            # Body text\n            if 'body_text' in content.keys():\n                for entry in content['body_text']:\n                    self.body_text.append(entry['text'])\n            else:\n                self.body_text.append('')\n                \n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n\n            # Extend Here\n            #\n            #\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(all_json[0])\nprint(first_row)\n\n\n","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"y4A2PuF9bBen"},"cell_type":"code","source":"dict_ = {'paper_id': [], 'abstract': [], 'body_text': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) // 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text'])\n#df_covid.head()","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"YSUB6oWPbBep"},"cell_type":"code","source":"# https://www.kaggle.com/maksimeren/covid-19-literature-clustering\n\ndict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) // 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 300 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_covid.head()","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"tRW_vcCibBeu"},"cell_type":"code","source":"df_covid1=df_covid.drop( [\"ents\"],axis=1)","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"2Kvszt3sbBew"},"cell_type":"code","source":"# export index\nfrom pathlib import Path\nfilepath = Path(\"/kaggle/working\") /\"df_covid\"\noutfile=open(filepath, \"wb\")\npickle.dump(df_covid,outfile)","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"juPq4a10bBez"},"cell_type":"code","source":"# Download model. Upload to datasets\nos.chdir(\"/kaggle/working\")\nfrom IPython.display import FileLink\nFileLink(r'df_covid')","execution_count":0,"outputs":[]},{"metadata":{"id":"xg2v8UyqbBe2"},"cell_type":"markdown","source":"### Part C\nPart C involves embedding the CORD-19 dataset using our fine-tuned SentenceTranformer model (from Part A), and setting up the faiss  index to store the embeddings."},{"metadata":{"trusted":true,"id":"S1a8uwPDbBe3"},"cell_type":"code","source":"os.chdir(\"/kaggle/working/sentence-transformers\")\n\nfrom sentence_transformers import SentenceTransformer\nos.chdir(\"/kaggle/working/\")","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"sDlbxDFsbBe5"},"cell_type":"code","source":"# Load model from input \nmodel_load_path='/kaggle/input/bertcovidbasicnlists/training_nli_sts_covid-bert-base-2020-04-01_00-26-48'\nmodel=torch.load(model_load_path) \n# type(model) #  model is of type \"SentenceTransformer\"","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"-1p2Pl4tbBe9"},"cell_type":"code","source":"# init faiss index\nimport faiss # in case not already loaded \nd=768 # sentence transformer embedding length\nres = faiss.StandardGpuResources()  # use a single GPU\nindex = faiss.IndexIDMap(faiss.IndexFlatIP(d)) # IP is inner product. Data must be normalised first\ngpu_index = faiss.index_cpu_to_gpu(res, 0, index)","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"vPzsGvzbbBfB"},"cell_type":"code","source":"# load covid_df\n# import dictionary\nfrom pathlib import Path\nfilepath = Path(\"/kaggle/input/covid-docs-processed-dataframe\") / \"df_covid_ents\"\ninfile=open(filepath, \"rb\")\ndf_covid=pickle.load(infile)","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"3khhYBMbbBfE"},"cell_type":"code","source":"df_covid.shape","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"VcDYDam8bBfH"},"cell_type":"code","source":"# embed documents and add them to faiss index \n# maintain a dictionary which stores, for each index in faiss, details of the paper (from papers_df)\n\n# index - ids, paper_id, sent_idx\n\nids_dict = collections.defaultdict(list)\n\nids_next=0\n\n#for row in range(papers_df.shape[0]):\nfor row in range(df_covid.shape[0]):\n\n    # doc=nlp(papers_df.iloc[row]['text'])\n    doc=nlp(df_covid.iloc[row]['body_text'])\n\n    # paper_id=papers_df.iloc[row]['paper_id']\n    paper_id=df_covid.iloc[row]['paper_id']\n\n    list_of_sents=[sent.text for sent in doc.sents]\n    doc_matrix=embed_sentence_list(model,list_of_sents)\n    faiss.normalize_L2(doc_matrix)\n\n    # ids in faiss index begin after last idx (last document processed, last sentence)\n    custom_ids = np.array(range(ids_next, ids_next+len(doc_matrix))) # from last postion (range add +1) to new position\n    ids_next=ids_next+len(doc_matrix) # increment by current document length. Current doc lenght = num sentences in current doc             \n    gpu_index.add_with_ids(doc_matrix, custom_ids)\n    for sent_idx, faiss_ids_val in enumerate(custom_ids): # sentence_idx is sentence id within the 1 document, faiss_ids_val is the faiss index value\n        items=(paper_id, sent_idx)\n        ids_dict[faiss_ids_val].append(items)\n","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"WhdnCn_DbBfJ"},"cell_type":"code","source":"\n","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"WfeXavktbBfL"},"cell_type":"code","source":"# serialize index for output \ncpu_index=faiss.index_gpu_to_cpu(gpu_index)\nindex_ser=serialize_index(cpu_index)","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"z_E7r9akbBfN"},"cell_type":"code","source":"# export index\nfrom pathlib import Path\nfilepath = Path(\"/kaggle/working\") /\"index_faiss_file\"\noutfile=open(filepath, \"wb\")\npickle.dump(index_ser,outfile)\n\n","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"lDN5YrePbBfS"},"cell_type":"code","source":"# export dictionary\nfrom pathlib import Path\nfilepath = Path(\"/kaggle/working\") /\"faiss_index_ids_dict\"\noutfile=open(filepath, \"wb\")\npickle.dump(ids_dict,outfile)","execution_count":0,"outputs":[]},{"metadata":{"trusted":true,"id":"AAK3UcUzbBfU"},"cell_type":"code","source":"# Download model. Upload to datasets\nos.chdir(\"/kaggle/working\")\nos.listdir()\nfrom IPython.display import FileLink\nFileLink(r'index_faiss_file')\nFileLink(r'faiss_index_ids_dict')","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"sentencetransf-covid-fine-tuning-embedding-index.ipynb","provenance":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}